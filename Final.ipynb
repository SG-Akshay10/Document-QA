{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05ebf948",
   "metadata": {},
   "source": [
    "# Document Based Question Answering System\n",
    "* Base Research Paper: https://arxiv.org/pdf/1805.08092.pdf\n",
    "* Other References:\n",
    "    * https://ieeexplore.ieee.org/abstract/document/9079274\n",
    "    * https://arxiv.org/pdf/1707.07328.pdf\n",
    "    * https://arxiv.org/pdf/1810.04805.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bae9476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5ecc30",
   "metadata": {},
   "source": [
    "### Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc02e65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import pdfplumber\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92fb6410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pprint\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import Word2Vec  \n",
    "import gensim.downloader as api  \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.parsing.preprocessing import remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0164196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77db978b",
   "metadata": {},
   "source": [
    "### Impoting Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f275b805",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = '/home/akshay/SNU_AI_DS/6th_sem/NLP/Project/DocumentQA/Main/Final/1.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3836ccd",
   "metadata": {},
   "source": [
    "### Extracting text from documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbd24979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_extract(pdf_path):\n",
    "    text = \"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text += page.get_text() + \"\\n\"\n",
    "    doc.close()\n",
    "\n",
    "    # Remove authors' names and specific dataset names\n",
    "    cleaned_text = re.sub(r'\\b[A-Z]+\\s[A-Z]\\s[A-Z]+(\\s-\\s[A-Z]\\s-\\s\\d+)\\b', '', text)\n",
    "    \n",
    "    # Remove section headings\n",
    "    cleaned_text = re.sub(r'\\b\\d+\\.\\s[A-Z]+\\b', '', cleaned_text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    \n",
    "    return cleaned_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e8d031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = pdf_extract(my_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e782af",
   "metadata": {},
   "source": [
    "### Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a2e7186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence, stopwords=False):\n",
    "    sentence = sentence.lower().strip() # Convert the sentence to lowercase and remove leading/trailing whitespaces\n",
    "    sentence = re.sub(r'[^a-z0-9\\s]', '', sentence) # Remove any characters that are not alphabets, digits, or whitespaces\n",
    "    if stopwords:\n",
    "        sentence = remove_stopwords(sentence) # Optionally remove stopwords from the sentence\n",
    "    return sentence\n",
    "\n",
    "def get_cleaned_sentences(tokens, stopwords=False):\n",
    "    cleaned_sentences = [] # Initialize an empty list to store cleaned sentences\n",
    "    for row in tokens: # Iterate over each row in the tokens\n",
    "        cleaned = clean_sentence(row, stopwords) # Clean the sentence using the clean_sentence function\n",
    "        cleaned_sentences.append(cleaned) # Append the cleaned sentence to the list of cleaned_sentences\n",
    "    return cleaned_sentences # Return the list of cleaned sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed42c29b",
   "metadata": {},
   "source": [
    "## Model based on Bag Of Words and Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "905499ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieveAndPrintFAQAnswer(question_embedding, sentence_embeddings, sentences):\n",
    "    max_sim = -1  # Initialize the maximum similarity score\n",
    "    index_sim = -1  # Initialize the index of the most similar sentence\n",
    "    for index, embedding in enumerate(sentence_embeddings):  # Iterate over the sentence embeddings\n",
    "        # Compute the cosine similarity between the question embedding and the current sentence embedding\n",
    "        sim = cosine_similarity(embedding, question_embedding)[0][0]\n",
    "        if sim > max_sim:  # If the current similarity is greater than the maximum similarity found so far\n",
    "            max_sim = sim  # Update the maximum similarity\n",
    "            index_sim = index  # Update the index of the most similar sentence\n",
    "  \n",
    "    return index_sim  # Return the index of the most similar sentence\n",
    "\n",
    "\n",
    "def naive_drive(file_name, question):\n",
    "    pdf_txt = pdf_extract(file_name)  # Extract text from the PDF file\n",
    "    tokens = nltk.sent_tokenize(pdf_txt)  # Tokenize the text into sentences\n",
    "    cleaned_sentences = get_cleaned_sentences(tokens, stopwords=True)  # Clean and preprocess the sentences\n",
    "    cleaned_sentences_with_stopwords = get_cleaned_sentences(tokens, stopwords=False)  # Clean sentences without removing stopwords\n",
    "    sentences = cleaned_sentences_with_stopwords  # Assign cleaned sentences to 'sentences'\n",
    "    sentence_words = [[word for word in document.split()] for document in sentences]  # Tokenize each sentence into words\n",
    "\n",
    "    dictionary = corpora.Dictionary(sentence_words)  # Create a dictionary from the tokenized words\n",
    "    bow_corpus = [dictionary.doc2bow(text) for text in sentence_words]  # Convert tokenized words into Bag-of-Words representation\n",
    "\n",
    "    question = clean_sentence(question, stopwords=False)  # Clean the question\n",
    "    question_embedding = dictionary.doc2bow(question.split())  # Convert the question into a Bag-of-Words representation\n",
    "\n",
    "    index = retrieveAndPrintFAQAnswer(question_embedding, bow_corpus, sentences)  # Retrieve the index of the most similar sentence\n",
    "    \n",
    "    return sentences[index]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f314a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the authors utilize pretrained bert models from ten sorflow hub enabling easy integration into their implementa tion\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the four different transformer-based embedding models?\"\n",
    "answer = naive_drive(my_path, question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8376b576",
   "metadata": {},
   "source": [
    "## Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efd04da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model successfully loaded\n"
     ]
    }
   ],
   "source": [
    "v2w_model = None  # Initializing Word2Vec model variable\n",
    "\n",
    "try:\n",
    "    v2w_model = gensim.models.KeyedVectors.load('./w2vecmodel.mod')  # Try to load the Word2Vec model from a local file\n",
    "    print(\"Word2Vec model successfully loaded\")  \n",
    "except FileNotFoundError:  # Handle the case when the local file is not found\n",
    "    v2w_model = api.load('word2vec-google-news-300')  # Load the pre-trained \"word2vec-google-news-300\" model from gensim downloader\n",
    "    v2w_model.save(\"./w2vecmodel.mod\")  # Save the loaded model to a local file for future use\n",
    "    print(\"Word2Vec model saved\")  \n",
    "\n",
    "w2vec_embedding_size = len(v2w_model['pc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e9189a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordVec(word, model):\n",
    "    samp = model['pc']\n",
    "    vec = [0]*len(samp)\n",
    "    try:\n",
    "        vec = model[word]\n",
    "    except:\n",
    "        vec = [0]*len(samp)\n",
    "    return (vec)\n",
    "\n",
    "\n",
    "def getPhraseEmbedding(phrase, embeddingmodel):\n",
    "    samp = getWordVec('computer', embeddingmodel)\n",
    "    vec = np.array([0]*len(samp))\n",
    "    den = 0;\n",
    "    for word in phrase.split():\n",
    "        den = den+1\n",
    "        vec = vec + np.array(getWordVec(word, embeddingmodel))\n",
    "    return vec.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b91cdce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_drive(file_name, question):\n",
    "    pdf_txt = pdf_extract(file_name)  # Extract text from the PDF file\n",
    "\n",
    "    tokens = nltk.sent_tokenize(pdf_txt)  # Tokenize the text into sentences\n",
    "    cleaned_sentences_with_stopwords = get_cleaned_sentences(tokens, stopwords=False)  # Clean sentences without removing stopwords\n",
    "    sentences = cleaned_sentences_with_stopwords  # Assign cleaned sentences to 'sentences'\n",
    "    sentence_words = [[word for word in document.split()] for document in sentences]  # Tokenize each sentence into words\n",
    "\n",
    "    sent_embeddings = []  # Initialize a list to store embeddings of sentences\n",
    "    for sent in sentences:  # Iterate over each sentence\n",
    "        sent_embeddings.append(getPhraseEmbedding(sent, v2w_model))  # Generate the embedding for the sentence and append it to the list\n",
    "\n",
    "    question_embedding = getPhraseEmbedding(question, v2w_model)  # Generate the embedding for the question\n",
    "    index = retrieveAndPrintFAQAnswer(question_embedding, sent_embeddings, cleaned_sentences_with_stopwords)  # Retrieve the index of the most similar sentence\n",
    "    return cleaned_sentences_with_stopwords[index]  # Return the most similar sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c825bb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "four different transformerbased embedding models are used  bert pretrained on a large text corpus and finetuned for specific tasks\n"
     ]
    }
   ],
   "source": [
    "answer = word2vec_drive(my_path, question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065dfc21",
   "metadata": {},
   "source": [
    "## Glove Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abb92197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove Model Saved\n"
     ]
    }
   ],
   "source": [
    "glove_model = None\n",
    "try:\n",
    "    glove_model = gensim.models.Keyedvectors.load('./glovemodel.mod')\n",
    "    print(\"Glove Model Successfully loaded\")\n",
    "except:\n",
    "    glove_model = api.load('glove-twitter-25')\n",
    "    glove_model.save(\"./glovemodel.mod\")\n",
    "    print(\"Glove Model Saved\")\n",
    "\n",
    "glove_embedding_size = len(glove_model['pc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b837e0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_drive(file_name, question):\n",
    "    pdf_txt = pdf_extract(file_name)\n",
    "\n",
    "    tokens = nltk.sent_tokenize(pdf_txt)\n",
    "    cleaned_sentences = get_cleaned_sentences(tokens, stopwords=True)\n",
    "    cleaned_sentences_with_stopwords = get_cleaned_sentences(tokens, stopwords=False)\n",
    "    sentences = cleaned_sentences_with_stopwords\n",
    "    sentence_words = [[word for word in document.split()] for document in sentences]\n",
    "\n",
    "    sent_embeddings = []\n",
    "    for sent in cleaned_sentences:\n",
    "        sent_embeddings.append(getPhraseEmbedding(sent, glove_model))\n",
    "\n",
    "    question_embedding = getPhraseEmbedding(question, glove_model)\n",
    "    index = retrieveAndPrintFAQAnswer(question_embedding, sent_embeddings, cleaned_sentences_with_stopwords)\n",
    "    return cleaned_sentences_with_stopwords[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e34019cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "close behind small bert  cnn and albert  mlp models produce an f1 score of 067 making them the next bestperforming models in the study\n"
     ]
    }
   ],
   "source": [
    "answer = glove_drive(my_path, question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11eb346",
   "metadata": {},
   "source": [
    "# BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7275167b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c50787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question_bert(question, answer_text):\n",
    "\n",
    "    input_ids = tokenizer.encode(question, answer_text, max_length=512, truncation=True)\n",
    "\n",
    "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "    num_seg_a = sep_index + 1\n",
    "\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "    start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids])).values()\n",
    "\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    #print(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\n",
    "    #print(f'score: {torch.max(start_scores)}')\n",
    "    score = float(torch.max(start_scores))\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    answer = tokens[answer_start]\n",
    "\n",
    "    for i in range(answer_start + 1, answer_end + 1):\n",
    "\n",
    "        if tokens[i][0:2] == '##':\n",
    "            answer += tokens[i][2:]\n",
    "        else:\n",
    "            answer += ' ' + tokens[i]\n",
    "        #if tokens[i][0:2] == ' ':\n",
    "         #   answer += tokens[i][2:]\n",
    "\n",
    "        #else:\n",
    "           # answer += ' ' + tokens[i]\n",
    "    return answer, score, start_scores, end_scores, tokens\n",
    "    #print('Answer: \"' + answer + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a98d86b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_split_sentences(pdf_text,max_tokens=256):\n",
    "    tokenized_text = []\n",
    "    sentences = nltk.sent_tokenize(pdf_text)\n",
    "    for sentence in sentences:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        tokenized_text.extend(tokens)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_chunk_tokens = 0\n",
    "    for token in tokenized_text:\n",
    "        current_chunk.append(token)\n",
    "        current_chunk_tokens += 1\n",
    "        if current_chunk_tokens >= max_tokens:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_chunk_tokens = 0\n",
    "    if current_chunk_tokens > 0:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "701c6bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_drive(ftext, question):\n",
    "    text = ftext\n",
    "    max_score = 0;\n",
    "    final_answer = \"\"\n",
    "    new_df = expand_split_sentences(text)\n",
    "    tokens = []\n",
    "    s_scores = np.array([])\n",
    "    e_scores = np.array([])\n",
    "    for new_context in new_df:\n",
    "    #new_paragrapgh = new_paragrapgh + answer_question(question, answer_text)\n",
    "        ans, score, start_score, end_score, token = answer_question_bert(question, new_context)\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            s_scores = start_score.detach().numpy().flatten()\n",
    "            e_scores = end_score.detach().numpy().flatten()\n",
    "            tokens = token\n",
    "            final_answer = ans\n",
    "    return final_answer, s_scores, e_scores, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74d0434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_drive(file_name, question):\n",
    "    # Extract text from PDF\n",
    "    text = pdf_extract(file_name)\n",
    "    \n",
    "    # Initialize variables\n",
    "    max_score = 0\n",
    "    final_answer = \"\"\n",
    "    new_df = expand_split_sentences(text)\n",
    "    tokens = []\n",
    "    s_scores = np.array([])\n",
    "    e_scores = np.array([])\n",
    "    \n",
    "    # Iterate over split sentences and find the best answer\n",
    "    for new_context in new_df:\n",
    "        ans, score, start_score, end_score, token = answer_question_bert(question, new_context)\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            s_scores = start_score.detach().numpy().flatten()\n",
    "            e_scores = end_score.detach().numpy().flatten()\n",
    "            tokens = token\n",
    "            final_answer = ans\n",
    "    \n",
    "    #return new_df\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de053f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert , electra , and albert\n"
     ]
    }
   ],
   "source": [
    "text = pdf_extract(my_path)\n",
    "\n",
    "answer = bert_drive(my_path, question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce02ab6",
   "metadata": {},
   "source": [
    "# Gradio UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7880155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e27121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = gr.File(label=\"Input File\")\n",
    "question_input = gr.Textbox(label=\"Question\")\n",
    "output_text = gr.Textbox(label=\"Answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e49d2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.Interface(fn=bert_drive,inputs=[input_file, question_input],outputs=output_text).launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
